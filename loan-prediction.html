<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Derek Whitaker — Loan Prediction</title>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css" integrity="sha256-46qynGAkLSFpVbEBog43gvNhfrOj+BmwXdxFgVK/Kvc=" crossorigin="anonymous" />

        <!-- Fonts -->
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,900|Source+Sans+Pro:300,900&display=swap" rel="stylesheet">

        <!-- Portfolio Stylesheet -->
        <link rel="stylesheet" href="css/style.css">

        <!-- Highlight code package -->
        <link rel="stylesheet" href="css/hl-styles/github.css">

    </head>
    <body>
        <header>
            <div class="logo">
                <img src="img/logo.png" alt="">
            </div>
            <button class="nav-toggle" aria-label="toggle navigation">
                <span class="hamburger"></span>
            </button>
            <nav class="nav">
                <ul class="nav__list">
                    <li class="nav__item"><a href="index.html" class="nav__link">Home</a></li>
                    <li class="nav__item"><a href="index.html#services" class="nav__link">My Services</a></li>
                    <li class="nav__item"><a href="index.html#about" class="nav__link">About me</a></li>
                    <li class="nav__item"><a href="index.html#work" class="nav__link">My Work</a></li>
                </ul>
            </nav>
        </header>


        <section class="intro" id="home">
            <h1 class="section__title section__title--intro">
                Neural Networks for Loans
            </h1>
            <p class="section__subtitle section__subtitle--intro">Using Deep Learning to Determine Financial Risk</p>
            <!-- Source of LendingClub Photo -->
            <!-- https://upload.wikimedia.org/wikipedia/commons/f/f3/Lending_club_picture_from_the_street.jpg -->
            <!-- Mediasagax / CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0) -->
            <img src="img/lending-header.jpg" alt="a picture of Lending Club in New York City" class="intro__img">
        </section>
        <section>
          <div class="portfolio-item-individual">

            <!-- Here it would be nice to have link buttons that go to each section of the page (e.g. intro, EDA, etc.) -->
            <!-- Links like what Aaron has on his KidsPool page -->

            <h2>Introduction</h2>
            <!-- potentially a button to show or hide the code -->
            <!-- <button onclick="showHide()">Show/Hide Code</button> -->

            <h3>0.1 Objective</h3>
            <p>Using data provided by the LendingClub, a peer-to-peer lending company, we sought to design a model that would analyze information known about a potential borrower and predict if this person would fully pay back their loan. </p>

            <h3>0.2 Dataset</h3>
            <p>We use a subset of a LendingClub dataset available on <a href="https://www.kaggle.com/wordsforthewise/lending-club">Kaggle</a>. This dataset contains information about the person requesting the loan, if their request was accepted or denied, and — if the request was accepted — whether the loan was paid back in full or charged off.</p>

            <p>The dataset contains 396030 entries and is organized into 27 columns containing a mix of categorical and numeric features.</p>

            <h3>0.3 Outcome</h3>
            <p>We used TensorFlow and Keras to design a deep learning neural network that was able to predict with 89% accuracy if a potential borrower would pay back their loan. Considering the unbalanced nature of dataset, which contains much almost 80% fully paid loans, this model presents a moderate level of accuracy which could potentially be approved upon with further development.</p>

            <!-- SECTION 1 -->
            <h2>1.0 Exploratory Data Analysis</h2>

            <p>We began by performing an initial analysis of the data in order to view summary statistics, visualize the data, and gain an understanding of potentially important data features.</p>

            <pre><code class="python" id="src-code">
              #Load the necessary Python libraries and frameworks
              import pandas as pd
              import numpy as np
              import matplotlib.pyplot as plt
              import seaborn as sns
              %matplotlib inline
            </code></pre>

            <pre><code class="python">
              #Load the CSV file as pandas data frame object
              df = pd.read_csv('../DATA/lending_club_loan_two.csv')

              # Loads a CSV file containing information about  each column of the dataframe.
              data_info = pd.read_csv('../DATA/lending_club_info.csv',index_col='LoanStatNew')
            </code></pre>

            <pre><code>
              #Creates a function that provides the description of the dataset’s columns.
              def feat_info(col_name):
                  print(data_info.loc[col_name]['Description'])
            </code></pre>

            <h3>1.1 Loan Status</h3>
            <p>Since we were interested in predicting loan status, we began by creating a count plot that shows how many loans were fully paid off and how many were charged off.</p>

            <pre><code class="python">
              plt.figure(figsize=(18,6))
              sns.countplot(df.loan_status)
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 1</p>
              <p class='fig-title'>Number of Loans per Loan Status</p>
              <img src="img/loan-prediction/fig1.png" alt="">
            </div>

            <p>There are considerably more fully paid off loans than there are charged off loans. This intuitively makes sense: we would expect defaults to be the exception. However, this meant we had an unbalanced dataset, which suggested that our predictive model would likely have a good accuracy. We would need to use precision and recall as the guiding metrics for determining our model’s strength.</p>

            <h3>1.2 Loan Frequency</h3>
            <p>Next, we created a histogram of loan sizes to see how frequent certain amounts are.</p>

            <pre><code class="python">
              plt.figure(figsize=(18,6))
              sns.distplot(df['loan_amnt'],kde=False,bins=40)
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 2</p>
              <p class='fig-title'>Number of Loans by Loan Amount</p>
              <img src="img/loan-prediction/fig2.png" alt="">
            </div>

            <p>We saw that there are spikes at even money amounts — $10,000, $15,000, $20,000, etc. — as opposed to random amounts. We can conclude that loans are frequently distributed in standard amounts.</p>

            <h3>1.3 Exploring Correlation</h3>
            <p>We then explored the correlation between the continuous feature variables by calculating these values and plotting them as a heat map.</p>

            <pre><code class="python">
              #cmap viridis to better visualzie the correlation.
              plt.figure(figsize=(18,6))
              sns.heatmap(df.corr(),annot=True,cmap='viridis')
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 3</p>
              <p class='fig-title'>Heat Map of Feature Correlations</p>
              <img src="img/loan-prediction/fig3.png" alt="">
            </div>

            <p>We noticed an almost perfect correlation between the ‘loan_amnt’ and ‘installment’ features. We sought to further explore this relationship to determine if these feature contain duplicate information  Using the feat_info function we designed at the beginning, we were able to ascertain that ‘installment’ is “The monthly payment owed by the borrower if the loan originates,” while ‘loan_amnt’ is “The listed amount of the loan applied for by the borrower.” It seemed probable that there would be a direct relationship between the loan amount and the size of monthly payment due on the loan. In order to explore this intuition, we created a scatterplot of the relation between the ‘loan_amnt’ and ‘installment’ features.</p>

            <pre><code class="python">
              plt.figure(figsize=(15,7))
              sns.scatterplot(data=df,x='installment',y='loan_amnt')
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 4</p>
              <p class='fig-title'>The Relation of Installment Size and Loan Ammount</p>
              <img src="img/loan-prediction/fig4.png" alt="">
            </div>

            <p>Our scatterplot supports this interpretation. It makes sense that the installment size would be tied to the loan amount. We can guess that the company probably uses some kind of formula to determine the installment payment based on the loan amount.</p>

            <h3>1.4 Loan Size</h3>
            <p>Next, we created a box plot to explore the relation between the size of the loan and how likely it is to be fully paid or charged off.</p>

            <pre><code class="python">
              plt.figure(figsize=(12,9))
              sns.boxplot(data=df,x='loan_status',y='loan_amnt')
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 5</p>
              <p class='fig-title'>Box Plot of Loan Amount by Loan Status</p>
              <img src="img/loan-prediction/fig5.png" alt="">
            </div>

            <p>The two loan statuses appear fairly similar. If loan amount is higher, there is a slightly higher chance of it being charged off. This intuitively makes sense: larger loans are harder to pay off. However, the difference between fully paid and charged off appears to be very slight. This small difference, while indicating some relationship, does not suggest that loan size is a key indicator in final loan status.</p>
            <p>We wanted to clarify this conclusion by finding the exact figures concerning the relationship between loan status and loan amount.</p>

            <pre><code class="python">
              df.groupby('loan_status')['loan_amnt'].describe()
            </code></pre>

            <div class="tab-fig-top">
              <P class="fig-number">Table 1</p>
              <p class="fig-title">Descriptive Statistics of Loan Amount by Loan Status</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th></th>
                  <th>count</th>
                  <th>mean</th>
                  <th>std</th>
                  <th>min</th>
                  <th>25%</th>
                  <th>50%</th>
                  <th>75%</th>
                  <th>max</th>
                </tr>
                <tr>
                  <td><strong>Charged off</strong></td>
                  <td>77673.0</td>
                  <td>15126.300967</td>
                  <td>8505.090557</td>
                  <td>1000.0</td>
                  <td>8525.0</td>
                  <td>14000.0</td>
                  <td>20000.0</td>
                  <td>40000.0</td>
                </tr>
                <tr>
                  <td><strong>Fully Paid</strong></td>
                  <td>318357.0</td>
                  <td>13866.878771</td>
                  <td>8302.319699</td>
                  <td>500.0</td>
                  <td>7500.0</td>
                  <td>12000.0</td>
                  <td>19225.0</td>
                  <td>40000.0</td>
                </tr>
              </table>
            </div>


            <p>The mean loan amount for charged off loans is slightly higher the mean loan amount of fully paid off loans, although only by about $1,200. This reinforces what we saw in our box plot. A larger loan is somewhat more likely to be charged off, but loan size does not appear to be a key indicator in predicting loan status.</p>

            <h3>1.5 Loan Grade</h3>
            <p>Our next step was to explore the grade and sub grade ratings that LendingClub attributed to the loans. Each loan could receive a possible alphabetic grade from A to G, as well as an alphanumeric sub grade from A1 to G5.</p>
            <p>We created a count plot which showed the loan status per loan grade in order to see if there was any differentiation between them.</p>

            <pre><code class="python">
              plt.figure(figsize=(18,6))
              sns.countplot(data=df,x='grade',hue='loan_status')
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 6</p>
              <p class='fig-title'>Count of Fully Paid and Charged Off Loans per Grade Category</p>
              <img src="img/loan-prediction/fig6.png" alt="">
            </div>

            <p>This graph shows a clear between the loan grade and it’s likelihood of being charged off: as the loan grade decreases, the chances of a default increase. For example, grade G loans are essentially just as likely to be fully paid as they are to be charged off. On the other hand, only a small percentage of grade A and B loans are charged off.</p>

            <p>To further explore this relationship, we created a similar count plot, showing the number of loans per grade, as well as another count plot which also differentiates based on loan status.</p>

            <pre><code class="python">
              plt.figure(figsize=(18,6))
              sns.countplot(data=df,x='sub_grade',palette='coolwarm',
                            order=['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5',
                                   'C1','C2','C3','C4','C5','D1','D2','D3','D4','D5',
                                   'E1','E2','E3','E4','E5','F1','F2','F3','F4','F5',
                                   'G1','G2','G3','G4','G5'])
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 7</p>
              <p class='fig-title'>Number of Loans per Loan Amount</p>
              <img src="img/loan-prediction/fig7.png" alt="">
            </div>

            <pre><code class="python">
              plt.figure(figsize=(18,6))
              sns.countplot(data=df,x='sub_grade',palette='coolwarm',
                            order=['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5',
                                   'C1','C2','C3','C4','C5','D1','D2','D3','D4','D5',
                                   'E1','E2','E3','E4','E5','F1','F2','F3','F4','F5',
                                   'G1','G2','G3','G4','G5'],
                           hue='loan_status')
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 8</p>
              <p class='fig-title'>Number of Fully Paid and Charged Off Loans per Sub Grade Category</p>
              <img src="img/loan-prediction/fig8.png" alt="">
            </div>

            <p>Once again we saw that the lower the grade of the loan, the less likely it was to be fully paid. The F and G sub grades appear to be particularly bad, so we created another count plot that only included these sub grades. This would provide a clearer and easier to read image of the relationship between sub grade and loan status.</p>

            <pre><code class="python">
              #We first create a subset of our main data frame.
              df_fg = df[(df['grade'] == 'F') | (df['grade']== 'G')]
              #Next, we plot it out.
              plt.figure(figsize=(18,6))
              sns.countplot(data=df_fg ,x='sub_grade',
                            order=['F1','F2','F3','F4','F5',
                                   'G1','G2','G3','G4','G5'],
                           hue='loan_status')
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 9</p>
              <p class='fig-title'>Number of Fully Paid and Charged Off Loans per F and G Sub Grade Categories</p>
              <img src="img/loan-prediction/fig9.png" alt="">
            </div>

            <p>This reinforces our pervious observation. This graph shows that G5 loans, the lowest sub grade, are just as likely to be fully paid as they are to be charged off.</p>

            <h3>1.6 Loan Status</h3>
            <p>Finally, we want to explore if there are any significant correlations between loan status and the numeric features in our data set. To do this we had to first create a new feature in data frame that converted the categorical loan status feature into a boolean value 1 for “Fully Paid” and 0 for “Charged Off”.</p>

            <pre><code class="python">
              # We create a function that will take in the value as a string and convert it into an integer, giving us a boolean value.
              def fully_paid(repaid):
                  if repaid == 'Fully Paid':
                      return 1
                  else:
                      return 0
            </code></pre>

            <pre><code class="python">
              # We apply this function to the 'loan_status' column of our data set.
              df['loan_repaid'] = df['loan_status'].apply(fully_paid)
              # We could have also used .map() to change values
              # For example:
              # df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})
            </code></pre>

            <p>Once we had created a boolean value for our label, we were able to create a bar plot that shows the correlation of the numeric feature to loan status.</p>

            <pre><code class="python">
              # We are sure to call .sort_values on our plot, to create a easier to read visualization.
              # We are also sure to drop off the "loan_repaid" column from our plot, since there will be perfect correlation and this will distort the visualization of the plot.
              plt.figure(figsize=(18,6))
              df.corr()['loan_repaid'].sort_values(ascending=True)[:-1].plot.bar()
            </code></pre>

            <div class="fig">
              <p class="fig-number">Figure 10</p>
              <p class='fig-title'>Correlation of Numeric Features to Fully Paid Loan Status</p>
              <img src="img/loan-prediction/fig10.png" alt="">
            </div>

            <p>We see that interest rate has the highest negative correlation with whether or not someone repays their loan. This intuitively makes sense since a higher interest rate could create a more onerous financial situation.</p>

            <!-- SECTION 2 -->
            <h2>2.0 Data Preprocessing</h2>
            <p>Having performed our initial exploratory data analysis, we moved on to preprocessing our data. In order to get the data set ready to develop a predictive model, we would need to first remove or fill in any missing data, remove unnecessary or repetitive features, and finally convert categorical string features into dummy variables.</p>

            <h3>2.1 Missing Data</h3>
            <p>Our first step in handling missing data was to determine how much data was actually missing. We created a series that displayed the total count of missing values per column.</p>

            <pre><code class="python">
              ser_null = df.isnull().sum()
              ser_null
            </code></pre>

            <div class="tab-fig-top">
              <p class="fig-number">Table 2</p>
              <p class="fig-title">Data Frame Columns With Missng Values</p>
            </div>

            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th>Column Title</th>
                  <th>Missing Values</th>
                </tr>
                <tr>
                  <td>emp_title</td>
                  <td>22927</td>
                </tr>
                <tr>
                  <td>emp_length</td>
                  <td>18301</td>
                </tr>
                <tr>
                  <td>title</td>
                  <td>1755</td>
                </tr>
                <tr>
                  <td>revol_util</td>
                  <td>276</td>
                </tr>
                <tr>
                  <td>mort_acc</td>
                  <td>37795</td>
                </tr>
                <tr>
                  <td>pub_rec_bankruptcies</td>
                  <td>535</td>
                </tr>
              </table>
            </div>


            <p>We saw that the ‘emp_title’, ‘emp_length’, ‘title’, ‘revol_util’, ‘mort_acc’, and ‘pub_rec_bankruptcies’ columns were all missing some data. To better understand how much data was missing, we calculated the percentage missing for each feature.</p>

            <pre><code class="python">
              ser_null.apply(lambda n: (n/ 396030)*100)
            </code></pre>

            <div class="tab-fig-top">
              <p class="fig-number">Table 3</p>
              <p class="fig-title">Data Frame Columns With Missng Values as a Percentage</p>
            </div>

            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th>Column Title</th>
                  <th>Missing Values</th>
                </tr>
                <tr>
                  <td>emp_title</td>
                  <td>5.789208</td>
                </tr>
                <tr>
                  <td>emp_length</td>
                  <td>4.621115</td>
                </tr>
                <tr>
                  <td>title</td>
                  <td>0.443148</td>
                </tr>
                <tr>
                  <td>revol_util</td>
                  <td>0.069692</td>
                </tr>
                <tr>
                  <td>mort_acc</td>
                  <td>9.543469</td>
                </tr>
                <tr>
                  <td>pub_rec_bankruptcies</td>
                  <td>0.135091</td>
                </tr>
              </table>
            </div>

            <p>We saw that ’emp_title’, ’emp_length’, and ’mort_acc’ were the most significant columns with missing data. In particular, 'mort_acc' stood out with almost 10% of rows missing this information. Others were less significant, for example ’revol_util’ and ‘pub_rec_bankruptcies’: these rows could be dropped without worrying about affecting our data set. We began working through the list from top to bottom in order to address the missing data.</p>

            <h4>2.1.1 emp_title</h4>

            <p>The column ‘emp_title’ provides “the job title supplied by the Borrower when applying for the loan.” Our first step was to determine how many unique employment titles there were. This would help us decide if we should drop this feature or fill it in with some other value.</p>

            <pre><code class="python">
              df['emp_title'].nunique()
            </code></pre>

            <p>We found that there were 173,105 unique employment titles in our data set, meaning almost half of the entries in our dataset had a unique job title. There are too many unique entries to covert this into a dummy variable feature. Since we would be unable to use this column in our predictive model, we decided to drop it from our data frame.</p>

            <pre><code class="python">
              df.drop('emp_title',axis=1,inplace=True)
            </code></pre>

            <h4>2.1.2 emp_length</h4>
            <p>Next, we turned our attention towards the ‘emp_length’ feature, which provides “Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.” We began by creating a count plot of the ‘emp_length’ categories.</p>
            <pre><code class="python">
              plt.tight_layout()
              plt.figure(figsize=(18,6))
              sns.countplot(data=df, x='emp_length',
                           order=['< 1 year','1 year','2 years','3 years','4 years',
                                  '5 years','6 years','7 years','8 years','9 years',
                                 '10+ years'])
            </code></pre>
            <div class="fig">
              <p class="fig-number">Figure 11</p>
              <p class='fig-title'>Total Number of Loans per Employment Length Category</p>
              <img src="img/loan-prediction/fig11.png" alt="">
            </div>
            <p>This graph shows that the 10+ years employment length group is the largest. This makes sense since if we assume that those who hav been working for a longer amount of time are more likely to have the financial means to take out a loan. Because of their personal situation, they may also be more likely to need to take out a loan (e.g. to pay for home repair).</p>
            <p>In order to determine if employment length was an important feature, we decided to to verify if this feature had a relationship with loan status.</p>
            <pre><code class="python">
              plt.tight_layout()
              plt.figure(figsize=(18,6))
              sns.countplot(data=df, x='emp_length',
                           order=['< 1 year','1 year','2 years','3 years','4 years',
                                  '5 years','6 years','7 years','8 years','9 years',
                                 '10+ years'],
                           hue='loan_status')
            </code></pre>
            <div class="fig">
              <p class="fig-number">Figure 12</p>
              <p class='fig-title'>Number of Fully Paid and Charged Off Loans per Employment Length Category</p>
              <img src="img/loan-prediction/fig12.png" alt="">
            </div>
            <p>Based off this plot, it is difficult to judge if there is any significant relationship between loan status and employment length. We therefore decided to calculate the percent of charged off loans per employment length category.</p>
            <p>We created two data frames, one for those who paid off their loans and one for those who charged them off. We then used these two data frames to determine the ratio of persons who charged off versus those who fully paid.</p>
            <pre><code class="python">
              fully_paid = df[df['loan_repaid'] == 1]['emp_length'].value_counts(ascending=True)

              defaulted = df[df['loan_repaid'] == 0]['emp_length'].value_counts(ascending=True)

              defaulted/fully_paid
            </code></pre>


            <div class="tab-fig-top">
              <p class="fig-number">Table 4</p>
              <p class="fig-title">Ratio of Charged Off Loans per Employment Length Category</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th>Length of Employment</th>
                  <th>Ratio</th>
                </tr>
                <tr>
                  <td>1 year</td>
                  <td>0.248649</td>
                </tr>
                <tr>
                  <td>10+ years</td>
                  <td>0.225770</td>
                </tr>
                <tr>
                  <td>2 years</td>
                  <td>0.239560</td>
                </tr>
                <tr>
                  <td>3 years</td>
                  <td>0.242593</td>
                </tr>
                <tr>
                  <td>4 years</td>
                  <td>0.238213</td>
                </tr>
                <tr>
                  <td>5 years</td>
                  <td>0.237911</td>
                </tr>
                <tr>
                  <td>6 years</td>
                  <td>0.233341</td>
                </tr>
                <tr>
                  <td>7 years</td>
                  <td>0.241887</td>
                </tr>
                <tr>
                  <td>8 years</td>
                  <td>0.249625</td>
                </tr>
                <tr>
                  <td>9 years</td>
                  <td>0.250735</td>
                </tr>
                <tr>
                  <td>< 1 year</td>
                  <td>0.260830</td>
                </tr>
              </table>
            </div>

            <p>We also calculated what percent of loans for each category were charged off.</p>
            <pre><code class="python">
              defaulted/(defaulted+fully_paid)
            </code></pre>
            <div class="tab-fig-top">
              <p class="fig-number">Table 5</p>
              <p class="fig-title">Percent of Charged Off Loans per Employment Length Category</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th>Length of Employment</th>
                  <th>Ratio</th>
                </tr>
                <tr>
                  <td>1 year</td>
                  <td>0.199135</td>
                </tr>
                <tr>
                  <td>10+ years</td>
                  <td>0.184186</td>
                </tr>
                <tr>
                  <td>2 years</td>
                  <td>0.193262</td>
                </tr>
                <tr>
                  <td>3 years</td>
                  <td>0.195231</td>
                </tr>
                <tr>
                  <td>4 years</td>
                  <td>0.192385</td>
                </tr>
                <tr>
                  <td>5 years</td>
                  <td>0.192187</td>
                </tr>
                <tr>
                  <td>6 years</td>
                  <td>0.189194</td>
                </tr>
                <tr>
                  <td>7 years</td>
                  <td>0.194774</td>
                </tr>
                <tr>
                  <td>8 years</td>
                  <td>0.199760</td>
                </tr>
                <tr>
                  <td>9 years</td>
                  <td>0.200470</td>
                </tr>
                <tr>
                  <td>< 1 year</td>
                  <td>0.206872</td>
                </tr>
              </table>
            </div>
            <p>We observed across the extremes that there is little variation from one group to the next. For any given category, there is between an 18 and 20 % chance that the loan will be charged off. There does not appear to be any significant relationship between length of employment and the final status of a loan. To further verify this, we can plot these percentages in a bar graph.</p>
            <pre><code class="python">
              length_percentages = defaulted/(defaulted+fully_paid)

              plt.figure(figsize=(18,6))
              length_percentages.plot(kind='bar')
            </code></pre>
            <div class="fig">
              <p class="fig-number">Figure 13</p>
              <p class='fig-title'>Percentage of Charged Off Loans per Employment Length Category</p>
              <img src="img/loan-prediction/fig13.png" alt="">
            </div>
            <p>In this graph we can clearly observe that there is only minimal variation between categories. This is somewhat surprising. Those who have worked more than 10 years have a slightly smaller charge off rate. However, the difference is not extreme enough to support keeping this feature. Therefore, because rates are so similar across all employment lengths, we decided to drop this feature.</p>
            <pre><code class="python">
              df.drop('emp_length',axis=1,inplace=True)
            </code></pre>

            <h4>2.1.3 title</h4>
            <p>Next, we examined the ’title’ column of data frame in order to see what kind of information it was providing.</p>
            <pre><code class="python">
              df['title'].head(10)
            </code></pre>
            <p>It looked like this was giving information to what we might see in the ‘purpose’ column, so we checked this column to see if there might be duplicate information.</p>
            <pre><code class="python">
              df['purpose'].head(10)
            </code></pre>
            <p>Our intuition proved to be right. These two columns provided essentially the same information. Because it is missing information and is essentially a duplicate of ‘purpose’ we dropped the ‘title’ column from our data frame.</p>
            <pre><code class="python">
              df.drop('title',axis=1,inplace=True)
            </code></pre>

            <h4>2.1.4 mort_acc</h4>
            <p>We then turned our attention to ‘mort_acc’ column, which had initially stood out to us because 10% of this data is missing. This column shows how many mortgage accounts each person has. We performed a value count on this column to get a better idea of what it looked like.</p>
            <pre><code class="python">
              df['mort_acc'].value_counts()
            </code></pre>
            <p>We saw that the majority of people have 0 montage accounts. While there are extremes in this column (one individual with 34 montage accounts!), we decided that we could keep them without skewing our data.</p>
            <p>This column had a very high amount of missing values — almost 10% of the column. This meant we couldn’t drop those rows or else we would be dropping 10% of our data. We considered therefore dropping the feature. However, the number of mortgage accounts seemed intuitively significant to our label of loan status. Moreover, since we were dealing with a numeric feature and only 10% of data was missing, it would be possible to fill in the missing data with estimate values. We decided to peruse this option; it would turn out to be one of the most technically challenging aspects of the whole project.</p>
            <p>To fill in them missing values in the ‘mort_acc’ column, we began by looking at what features it was the most correlated with and find that a strong correlation with ‘total_acc,’ which stands for the total amount of accounts the person holds. This makes sense: the number of total accounts should be directly related to the number of mortgage accounts.</p>
            <pre><code class="python">
              df.corr()['mort_acc'].sort_values(ascending=True)
            </code></pre>
            <div class="tab-fig-top">
              <p class="fig-number">Table 8</p>
              <p class="fig-title">Correlation between Number of Mortage Accounts and Numeric Features</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th>Feature</th>
                  <th>Correlation</th>
                </tr>
                <tr>
                  <td>int_rate</td>
                  <td>-0.082583</td>
                </tr>
                <tr>
                  <td>dti</td>
                  <td>-0.025439</td>
                </tr>
                <tr>
                  <td>revol_util</td>
                  <td>0.007514</td>
                </tr>
                <tr>
                  <td>pub_rec</td>
                  <td>0.011552</td>
                </tr>
                <tr>
                  <td>pub_rec_bankruptcies</td>
                  <td>0.027239</td>
                </tr>
                <tr>
                  <td>loan_repaid</td>
                  <td>0.073111</td>
                </tr>
                <tr>
                  <td>open_acc</td>
                  <td>0.109205</td>
                </tr>
                <tr>
                  <td>installment</td>
                  <td>0.193694</td>
                </tr>
                <tr>
                  <td>revol_bal</td>
                  <td>0.194925</td>
                </tr>
                <tr>
                  <td>loan_amnt</td>
                  <td>0.222315</td>
                </tr>
                <tr>
                  <td>annual_inc</td>
                  <td>0.236320</td>
                </tr>
                <tr>
                  <td>total_acc</td>
                  <td>0.381072</td>
                </tr>
                <tr>
                  <td>mort_acc</td>
                  <td>1.000000</td>
                </tr>
              </table>
            </div>



            <p>To begin filling in the missing data, we grouped persons by total accounts and found the mean amount of mortgage accounts for each category. This provided us with an average that we could use to fill in the missing values. Once we had the mean mortgage account amount per total account amount, we created a function that would check to see if a row was missing a montage account value. If it did, it would find the mean mortgage account value for its total account value and then use this value to fill in the missing data. If the row already had a ‘mort_acc’ value, the function did nothing. After designing and testing our function to make sure it worked properly, we used a lambda expression to apply it to the two columns of our dataframe. This worked correctly, and we were able to fill in reasonable estimate values for about 10% of rows that were missing this data.</p>
            <pre><code class="python">
              total_acc_avg = df.groupby('total_acc').mean()['mort_acc']

              from math import isnan

              def fill_mort_acc(total_acc,mort_acc):
                  if np.isnan(mort_acc):
                      return total_acc_avg[total_acc]
                  else:
                      return mort_acc

              #With help from https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe
              df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)
            </code></pre>

            <h4>2.1.5 revol_util and pub_rec_bankruptcies</h4>
            <p>At this point, ‘revol_util’ and ‘pub_rec_bankruptcies’ were the only remaining columns with missing data. However, since these entries constituted less than 0.5% of the total, we decided to simply remove the rows who were missing this data.</p>
            <pre><code class="python">
              df = df.dropna()
            </code></pre>

            <h3>2.2 Categorical Variables and Dummy Variables</h3>
            <p>We completed cleaning up the missing data and could deal with the columns containing categorical variables. These would need to be converted into dummy variables, continuous numeric variables, or, if neither of these were possible, removed from the data frame. Our first step was to list what columns contained categorical data.</p>
            <pre><code class="python">
              list(df.select_dtypes(include=['object']).columns)
            </code></pre>

            <p>This indicated the following columns contained categorical data: 'term','grade', 'sub_grade', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'purpose', 'earliest_cr_line', 'initial_list_status', 'application_type', and'address'</p>

            <h4>2.2.1 Term</h4>
            <p>We began by looking at the ‘term’ feature and learned that this column shows the number of payments on the loan, which are listed as a string value of either ’36 months’ or ’60 months’. This was essentially already numeric data and we could easily convert this into a numeric variable: ’36 months’ would be become the integer 36, while ’60 months’ would become the integer 60. We made this change through the use of the .map() function.</p>
            <pre><code class="python">
              df['term'] = df['term'].map({' 36 months':36,' 60 months':60})
            </code></pre>

            <h4>2.2.2 grade and sub_grade</h4>
            <p>Our next task was to handle the ‘grade’ and ‘sub_grade’ columns. From our earlier exploratory data analysis, we had determined that the ‘grade’ column essentially contains duplicate information of the ‘sub_grade’ column, although in a less detailed format. Therefore, it made sense to drop the ‘grade’ column and only keep the ‘sub_grade’ feature.</p>
            <pre><code class="python">
              df.drop('grade',axis=1,inplace=True)
            </code></pre>
            <p>Once we had dropped the grade feature, we set up the sub grade feature as a dummy variable. To do this, we called the Pandas .get_dummies() function on the ‘sub_grade’ column, making sure to drop the first column. We then used the .concat() function to concatenate our new dummy variable columns onto our original data frame. At the same time, we dropped the original sub grade categorical data, keeping only the dummy variables.</p>
            <pre><code class="python">
              subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)

              df = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)
            </code></pre>

            <h4>2.2.3 verification_status, application_type, initial_list_status, and purpose</h4>
            <p>At this point we returned to our data frame to assess the remaining categorical data columns. We saw that 'verification_status', 'application_type', 'initial_list_status', and 'purpose' lent themselves well to creating dummy variables, since they all had only a few unique values in them, including some which were merely binary categories. Therefore, we created dummy variables from these columns as we had down with the ‘sub_grade’ column, concatenated them to our data frame, and dropped the orignal categorical columns.</p>
            <pre><code class="python">
              dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)
              df = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)
              df = pd.concat([df,dummies],axis=1)
            </code></pre>

            <h4>2.2.4 home_ownership</h4>
            <p>We also saw that the ‘home_ownership’ column would work well as a dummy variable because there were only a few unique values. However, before we could convert them we had consolidate some of the categories. There were essentially 3 main categories: ’MORTGAGE’, ’RENT’, and ‘OWN’. There were 3 other categories, which would work best grouped together: ‘OTHER’, ’NONE’, and ‘ANY’. Therefore, we used the .replace() function to merge these last three categories into one. We then proceeded to create dummy variables, concatenate them to the data frame, and drop the original categorical data.</p>
            <pre><code class="python">
              df['home_ownership'].value_counts()
            </code></pre>
            <div class="tab-fig-top">
              <p class="fig-number">Table 9</p>
              <p class="fig-title">Value Counts for Unique Values in Home Ownership Column</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th>Value</th>
                  <th>Count</th>
                </tr>
                <tr>
                  <td>MORTAGE</td>
                  <td>198022</td>
                </tr>
                <tr>
                  <td>RENT</td>
                  <td>159395</td>
                </tr>
                <tr>
                  <td>OWN</td>
                  <td>37660</td>
                </tr>
                <tr>
                  <td>OTHER</td>
                  <td>110</td>
                </tr>
                <tr>
                  <td>NONE</td>
                  <td>29</td>
                </tr>
                <tr>
                  <td>ANY</td>
                  <td>3</td>
                </tr>
              </table>
            </div>

            <p>Therefore, we used the .replace() function to merge these last three categories into one.  This left us with 4 categories: ‘MORTAGE’, ‘RENT’, ‘OWN’, and ‘OTHER’. We then proceeded to create dummy variables, concatenate them to the data frame, and drop the original categorical data.</p>
            <pre><code class="python">
              home_ownership_dummies = pd.get_dummies(df['home_ownership'].replace(['NONE','ANY'],'OTHER'),drop_first=True)
              df = df.drop('home_ownership',axis=1)
              df = pd.concat([df,home_ownership_dummies],axis=1)
            </code></pre>

            <h4>2.2.5 address</h4>
            <p>The address column of our data frame provided the full address of the person as a string value. We decided that the best way to use this information would be to extract the zip code from the string and convert it to a dummy variable. This proved a reasonable solution since there were only 10 unique zip code values.</p>
            <pre><code class="python">
              df['zip_code'].value_counts()
            </code></pre>
            <pre><code class="python">
              # We extract the zip code from the address feature.
              df['zip_code'] = df['address'].apply(lambda x: x.split()[-1])

              # We create dummy variables form the zip codes.
              zip_dummy = pd.get_dummies(df['zip_code'],drop_first=True)

              # We drop the original categorical data and concatenate the new dummy variables onto our data frame.
              df = df.drop(['zip_code','address'],axis=1)
              df = pd.concat([df,zip_dummy],axis=1)
            </code></pre>

            <h4>2.2.6 issue_d</h4>
            <p>When we looked at the information contained in the ‘issue_d’ column, we saw it indicates the month the loan was funded. However, this would constitute data leakage, since in our model we wouldn’t know beforehand whether or not a loan would be issued. We are trying to specify based off of someone’s already known feature whether or not someone will pay off a loan — and therefore if a loan should be issued in the first place. Since this would be data leakage, we need to drop this column from our data frame.</p>
            <pre><code class="python">
              df = df.drop('issue_d',axis=1)
            </code></pre>

            <h4>2.2.7 earliest_cr_line</h4>
            <p>This feature contains an historical time stamp showing ‘the month the borrower's earliest reported credit line was opened.” The value is stored as a string with a month and a year (e.g. “Jun-1990”). We decided that the best thing to do here was to extract the year from the string value, convert it into an integer and use this number to create a continuous value variable. We did this with a lambda expression that split of the year from the string and turned this into a new column. We then dropped the ‘earliest_cr_line’ column from our data set.</p>
            <pre><code class="python">
              df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda x: int(x.split('-')[1]))
              df['earliest_cr_year']

              df = df.drop('earliest_cr_line',axis=1)
            </code></pre>

            <h4>2.2.8 loan_status</h4>
            <p>Finally, all that remains is the ‘loan_status’ column. We can drop this column, since it is a duplicate of the ‘loan_repaid’ column we had created earlier.</p>
            <pre><code class="python">
              df = df.drop('loan_status',axis=1)
            </code></pre>

            <h2>3.0 Creating and evaluating our model</h2>

            <h3>3.1 Train/Test Split</h3>
            <p>After having taken care of missing data and categorical data, we prepared our data frame to train and test a predictive model. We used the ‘loan_repaid’ as our prediction label, while the other columns served to create the predictive model. We then used a MinMaxScaler to normalize our data, making sure to only fit our training data.</p>
            <pre><code class="python">
              from sklearn.model_selection import train_test_split

              # features
              X = df.drop('loan_repaid',axis=1).values
              # labels
              y = df['loan_repaid'].values

              from sklearn.preprocessing import MinMaxScaler

              scaler = MinMaxScaler()

              X_train = scaler.fit_transform(X_train)

              X_test = scaler.transform(X_test)
            </code></pre>

            <h3>3.2 Model Creation in Keras</h3>
            <p>Once we had normalized the data, we were ready to create our model. To do this we used the Keras library with TensorFlow to design a deep neural network. Following best practices, we started with 78 neurons in our first layer, based on the number of features in data frame. We used an rectified linear unit as our activation function, and we added a dropout layer to prevent overfitting. We then created two hidden layers that each reduced their number of neurons by half of the preceding layer (so 39 and 19 neurons each, respectively). These layers both had a dropout layer and used the rectified linear unit as the activation function. Finally, our output layer had 1 neuron, since we were performing a binary classification. We used a sigmoid activation function to push values to be between 0 and 1. We then complied the model, with our loss set to binary cross entropy; our optimizer was set to ‘adam’.</p>
            <pre><code class="python">
              model = Sequential()

              model.add(Dense(78,activation='relu'))
              model.add(Dropout(0.2))
              #We match the number of neurons in our first layer to the number of features in the dataframe.
              # We use an rectified linear unit as our activation function.
              # We add a droupout layer to prevent overfitting.

              model.add(Dense(39,activation='relu'))
              model.add(Dropout(0.2))
              #We add in a hidden layer, reducing the number of neurons by half.

              model.add(Dense(19,activation='relu'))
              model.add(Dropout(0.2))
              #We add in another hidden layer, reducing the number of neurons by half.
              # Each hidden layer also has a dropout.

              #binary classification, we only have one neuron.
              # The sigmoid activation function will push values to be between 0 and 1
              model.add(Dense(1,activation='sigmoid'))

              # We compile the model
              # Because we are performing a binary classification, our loss is set to binary crossentropy.
              # We set our optimizer to 'adam'.
              model.compile(loss='binary_crossentropy',optimizer='adam')
            </code></pre>
            <p>Next we fitted our model using the training data. Because this was a large data set, we feed in the data in batches of 256. We included our validation data to see if the model is being overfitted. Our model fitting ran for 25 epochs.</p>

            <h3>3.3 Evaluating Model Performance</h3>
            <p>We plotted validation loss versus the training loss. Our training loss and validation loss decreased but only up to a certain point.</p>
            <pre><code class="python">
              model_loss = pd.DataFrame(model.history.history)
              model_loss
            </code></pre>
            <pre><code class="python">
              model_loss.plot(figsize=(15,6))
            </code></pre>
            <div class="fig">
              <p class="fig-number">Figure 14</p>
              <p class='fig-title'>Validaton Loss and Training Loss</p>
              <img src="img/loan-prediction/fig14.png" alt="">
            </div>
            <p>We then proceeded to create predictions from our X_test set.</p>
            <pre><code class="python">
              predictions = model.predict_classes(X_test)

              from sklearn.metrics import classification_report,confusion_matrix
              print(classification_report(y_test,predictions))
              print('\n')
              print(confusion_matrix(y_test,predictions))
            </code></pre>
            <div class="tab-fig-top">
              <p class="fig-number">Table 10</p>
              <p class="fig-title">Classifcation Report</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th></th>
                  <th>precision</th>
                  <th>recall</th>
                  <th>f1-score</th>
                  <th>support</th>
                </tr>
                <tr>
                  <td><strong>0</strong></td>
                  <td>0.98</td>
                  <td>0.44</td>
                  <td>0.61</td>
                  <td>15658</td>
                </tr>
                <tr>
                  <td><strong>1</strong></td>
                  <td>0.88</td>
                  <td>1.00</td>
                  <td>0.93</td>
                  <td>63386</td>
                </tr>
                <tr>
                  <td><strong>accuracy</strong></td>
                  <td></td>
                  <td></td>
                  <td>0.89</td>
                  <td>79044</td>
                </tr>
                <tr>
                  <td><strong>macro avg</strong></td>
                  <td>0.93</td>
                  <td>0.72</td>
                  <td>0.77</td>
                  <td>79044</td>
                </tr>
                <tr>
                  <td><strong>weighted avg</strong></td>
                  <td>0.90</td>
                  <td>0.89</td>
                  <td>0.87</td>
                  <td>79044</td>
                </tr>
              </table>
            </div>
            <div class="tab-fig-top">
              <p class="fig-number">Table 11</p>
              <p class="fig-title">Confusion Matrix</p>
            </div>
            <div style="overflow-x:auto;">
              <table>
                <tr>
                  <th></th>
                  <th></th>
                </tr>
                <tr>
                  <td>6899</td>
                  <td>8759</td>
                </tr>
                <tr>
                  <td>150</td>
                  <td>63236</td>
                </tr>
              </table>
            </div>
            <p>Here, accuracy represents the actual percent that our model correctly predicted. We have about a 90% accuracy. However, this is an unbalanced dataset, with much more loans being fully paid versus being charged off. If we had created a very simple model that predicted that any loan would be repaid, it would still be about 80% accurate. Our 89% accuracy is ok but not excellent. It presents only a 9% avantage over a simple model that predicts that all loans will be repaid. A better metric here is our model’s precision and recall, as well as the f1-score, which takes the harmonic mean of precision and recall. We see for example that our model is correctly predicting that a loan will be charged off 44% of the time.</p>
            <p> It’s possible that our model could be improved by changing some of its parameters. We might try adding more neurons or layers, for example. We could also try more feature engineering. One column we could not use was employment title. It would be labor intensive, but it might be possible to feature engineer dummy variables if were could categorize job titles based on an estimated average salary. However, this would be very speculative and might not be the best option considering the amount of time it would likely require.</p>

            <h2>4.0 Conclusion</h2>
            <p>We created a model that is capable of predicting whether or not a person is likely to fully pay off a loan with 89% accuracy. While the model is less performant when predicting which loans will be charged off, it still performs better than other possible models (“all loans will be repaid”) or a random guess. This makes it a useful tool that could help the company in determing the risk profile of an individual. For this reason, it merits more exploration to see if it can be further developed to improve prediction accuracy.</p>
          </div>
        </section>

        <!-- Footer -->
        <footer class="footer">
           <a href="mailto:whitaker.derek@gmail.com" class="footer__link">whitaker.derek@gmail.com</a>
            <ul class="social-list">
              <li class="social-list__item">
                  <a class="social-list__link" href="https://codepen.io">
                      <i class="fab fa-codepen"></i>
                  </a>
                </li>
                <li class="social-list__item">
                   <a class="social-list__link" href="http://dribbble.com">
                      <i class="fab fa-dribbble"></i>
                   </a>
                </li>
                <li class="social-list__item">
                    <a class="social-list__link" href="https://twitter.com">
                        <i class="fab fa-twitter"></i>
                    </a>
                </li>
                <li class="social-list__item">
                    <a class="social-list__link" href="https://github.com">
                        <i class="fab fa-github"></i>
                    </a>
                </li>
            </ul>
        </footer>


      <script src="js/index.js"></script>
      <script src="js/highlight.pack.js"></script>
      <script>hljs.initHighlightingOnLoad();</script>
    </body>
</html>
